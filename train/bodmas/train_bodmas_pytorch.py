"""
Train binary classifier on the bodmas dataset.
Save the trained model as pytorch and onnx.
"""


import torch
from torch import nn
import argparse
import matplotlib.pyplot as plt
import numpy as np
import sys
import os
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
#
# Gobal vars
#
DATAPATH = '../../archive/bodmas.npz'
SAVEDIR = "../../models/bodmas"
MODE = "binary"
N_CLASS = 2
DENSE = 4
EPOCHS = 40
BATCH_SIZE = 128
SEED = 14

np.random.seed(SEED)
torch.manual_seed(SEED)

def get_args():
    """
    Parse Arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--datapath", type=str, default=DATAPATH)
    parser.add_argument("-n", "--name", type=str, default = None)
    parser.add_argument("--savedir", type=str, default=SAVEDIR)
    #
    # Training Parameters
    #
    parser.add_argument("-l", "--dense", type=int, default=DENSE)
    parser.add_argument("-e", "--epochs", type=int, default=EPOCHS)
    parser.add_argument("-bs", "--batch_size", type=int, default=BATCH_SIZE)
    
    args = parser.parse_args()
    return args


class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleNN, self).__init__()
        if hidden_size == 0:
            self.fc = nn.Linear(input_size, 2)
        else:
            self.fc = nn.Sequential(
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Linear(hidden_size, 2)
            )

    def forward(self, x):
        return self.fc(x)


def load_and_split_data(filename, test_size=0.2, seed=12):
    """
    Load the data from a file, scale it, and split it into training and testing sets.

    :param filename: The path to the file containing the data.
    :param test_size: The proportion of the dataset to include in the test split.
    :param seed: The seed for random number generation.

    :return: Tensors X_train, X_test, y_train, y_test.
    """
    #
    # Load and scale data
    #
    scaler_standard = StandardScaler()
    data = np.load(filename)

    X = data['X']  # all the feature vectors
    y = data['y']  # labels, 0 as benign, 1 as malicious
    #
    # Scale data
    #
    scaler_standard.fit(X)
    X_scaled = scaler_standard.transform(X)
    #
    # Split the data
    #
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=seed)
    #
    # Convert to PyTorch tensors
    #
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long)

    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor


def train_model(model, X_train, y_train, X_test, y_test, args):
    """
    Train model
    """
    #
    # Init optimizer and loss function
    #
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  
    criterion = nn.CrossEntropyLoss() 
    #
    # Train
    #
    for epoch in range(args.epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

    #
    # Test accuracy
    #
    with torch.no_grad():
        outputs = model(X_test)
    acc = [1 if torch.argmax(o) == y else 0 for o, y in zip(outputs, y_test)]
    print(f"Accuracy: {sum(acc)/len(outputs):2f}")
    print(f"Final loss: {loss.item()}")
    #
    # Save Model
    #
    os.makedirs(args.savedir, exist_ok=True)
    torch.save(model.state_dict(), f'{args.savedir}/{args.name}.pt')
    #
    # Export model to onnx
    #
    torch.onnx.export(model, 
                  torch.randn(1, X_train.shape[1]), 
                  f"{args.savedir}/{args.name}.onnx", 
                  input_names=['input'], 
                  output_names=['output'],
                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
                  opset_version=11)
    
    print(f"Model saved in PyTorch format at: {args.savedir}/{args.name}.pt")
    print(f"Model also saved in ONNX format at: {args.savedir}/{args.name}.onnx")


# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Reproduce Main Function
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
if __name__ == "__main__":
    #
    # Get and validate arguments
    #
    args = get_args()
    #
    # Generate Training Set
    #
    print("\nLoading Data ...")
    X_train, X_test, y_train, y_test = load_and_split_data(args.datapath, test_size=0.2, seed=SEED)
    
    name = "none" if args.dense == 0 else args.dense
    args.name = f"malware_bodmas_binary_scaled_{name}-2"
    #
    # Training
    #
    print("\n-------- Training Parameters --------- ")
    print(f"NAME: {args.name}")
    print(f"DENSE: {args.dense}")
    print(f"EPOCHS: {args.epochs}")
    print(f"SEED: {SEED}")
    
    print("\n-------- Training --------- ")
    model = SimpleNN(X_train.shape[1], args.dense)
    train_model(model, X_train, y_train, X_test, y_test, args)