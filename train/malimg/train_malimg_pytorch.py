"""
Train malimg pytorch model.
"""

from torch import nn
from torch import optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import numpy as np
import random
import torch
import os
import argparse
import tqdm
import torch.onnx

#
# Gobal vars
#
DATAPATH = "../../archive/malimg_dataset/"
SAVEDIR = "../../models/malimg"
CONV1 = 32
IMSIZE = 64
EPOCHS = 10
N_CLASS = 25
BATCH_SIZE = 64
SEED = 25

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

def get_args():
    """
    Parse Arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--classes", type=int, default=N_CLASS, help="total number of class for nullclass mode.")
    parser.add_argument("-d", "--datapath", type=str, default=DATAPATH)
    parser.add_argument("-n", "--name", type=str, default = None)
    parser.add_argument("--savedir", type=str, default=SAVEDIR)
    #
    # Training Parameters
    #
    parser.add_argument("-s", "--imsize", type=int, default=IMSIZE)
    parser.add_argument("-c1", "--conv1", type=int, default=CONV1)
    parser.add_argument("-e", "--epochs", type=int, default=EPOCHS)
    parser.add_argument("-bs", "--batch_size", type=int, default=BATCH_SIZE)
    
    args = parser.parse_args()
    return args

def load_data(datapath, imsize, batch_size):
    """
    Load malimg dataset. 
    """
    transform = transforms.Compose([
        transforms.Grayscale(),
        transforms.ToTensor(),
        transforms.Resize((imsize, imsize), antialias=True),
    ])

    train_dataset = datasets.ImageFolder(root=os.path.join(datapath, "train"), transform=transform)
    test_dataset = datasets.ImageFolder(root=os.path.join(datapath, "validation"), transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader

class CNN(nn.Module):
    def __init__(self, channels, conv1, num_classes):
        super(CNN, self).__init__()
        self.conv1 = conv1
        if conv1 != 0:
            self.conv = nn.Conv2d(channels, conv1, kernel_size=3)
        else:
            self.conv = None
        self.flatten = nn.Flatten(start_dim=1) # Flatten layer --> must use for onnx

        output_size = IMSIZE * IMSIZE * channels if self.conv is None else conv1 * (IMSIZE - 2) * (IMSIZE - 2)
        self.fc = nn.Linear(output_size, num_classes)

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
            x = nn.functional.relu(x)
            
        x = self.flatten(x) # Using flatten layer --> must use for onnx
        x = self.fc(x)
        return x

## if you want to save the model with softmax
class ModelWithSoftmax(nn.Module):
    def __init__(self, original_model):
        super(ModelWithSoftmax, self).__init__()
        self.original_model = original_model

    def forward(self, x):
        x = self.original_model(x)
        return nn.functional.softmax(x, dim=1)

def train(train_loader, test_loader, args):
    """
    Train malimg
    """
    #
    # Init
    # 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CNN(1, args.conv1, args.classes).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    model.train()
    #
    # Train
    #
    for epoch in range(args.epochs):
        tot_loss = 0
        for data, target in tqdm.tqdm(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            tot_loss += loss.item()

        print(f"[Epoch {epoch+1}] Loss: {tot_loss/len(train_loader):.2f}")
    #
    # testing
    #
    print("\n Testing ...")
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    print(f"[Test Set] Average loss: {test_loss:.4f} |  Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)")
    #
    # Save pytorch  model
    #
    os.makedirs(args.savedir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(args.savedir, f"{args.name}.pt"))
    # 
    # Save onnx model
    #
    model.eval()
    dummy_input = torch.randn(1, 1, args.imsize, args.imsize, device=device) # Generate a dummy input
    onnx_path = os.path.join(args.savedir, f"{args.name}.onnx")
    torch.onnx.export(model, 
                      dummy_input, 
                      onnx_path, 
                      opset_version=11, 
                      input_names=['input'], 
                      output_names=['output'], 
                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})

    print(f"Model saved in PyTorch format at: {args.savedir}/{args.name}.pt")
    print(f"Model also saved in ONNX format at: {onnx_path}")



if __name__ == "__main__":
    #
    # Get and validate arguments
    #
    args = get_args()
    convs_sizes = [0, 4, 16]
    #
    # Generate Training Set
    #
    print("\nLoading Data ...")
    train_loader, test_loader = load_data(datapath=args.datapath, imsize=args.imsize, batch_size=args.batch_size)
    #
    # 
    #
    for s in convs_sizes:
        args.conv1 = s
        name = "linear" if args.conv1 == 0 else args.conv1
        args.name = f"malware_malimg_family_scaled_{name}-25"
        #
        # Training
        #
        print("\n-------- Training Parameters --------- ")
        print(f"NAME: {args.name}")
        print(f"IMSIZE: {args.imsize}")
        print(f"CONV1: {args.conv1}")
        print(f"EPOCHS: {args.epochs}")
        print(f"SEED: {SEED}")
        
        print("\n-------- Training --------- ")
        train(train_loader, test_loader, args)